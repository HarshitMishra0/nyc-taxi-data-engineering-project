# NYC Taxi Data Engineering Project ğŸš•ğŸ“Š

An End-to-End Azure-based Data Engineering project leveraging:
- **Azure Data Factory** for ingestion using Dynamic Pipelines
- **Azure Databricks** for transformations with PySpark
- **Delta Lake** for ACID-compliant storage
- **Medallion Architecture**: Bronze â†’ Silver â†’ Gold layers
- **Parquet File Format** for optimized big data storage

---

## ğŸ“ Project Structure

- `1_data_ingestion/`: ADF dynamic pipelines and JSON exports
- `2_data_transformation/`: PySpark scripts for processing
- `3_data_lake/`: Sample outputs in Parquet format
- `4_visuals_and_docs/`: Diagrams and architecture notes

---

## âš™ï¸ Tech Stack

- Azure Data Factory
- Azure Databricks
- PySpark
- Delta Lake
- Parquet
- Medallion Architecture
- Azure Data Lake Storage (Gen2)

---

## ğŸ“Š Architecture Overview

![architecture_diagram](./4_visuals_and_docs/architecture_diagram.png)

---

## ğŸ§  Key Concepts Covered

- Dynamic pipelines using `ForEach`, `Parameters`, and Linked Services
- API-based data ingestion (no manual uploads)
- PySpark transformations with Delta Lake time travel and versioning
- Medallion architecture best practices
- End-to-end project simulation for interview prep

---

## ğŸ“ Sample Data Flow

`API â†’ ADF â†’ ADLS (Bronze) â†’ Databricks (Silver) â†’ Delta Tables (Gold)`

---

## ğŸ“Œ How to Run

1. Clone the repo
2. Deploy the ADF JSON to your ADF instance
3. Open Databricks and import notebooks
4. Run the pipeline step by step (Raw â†’ Silver â†’ Gold)

---

## âœ… Status

âœ… Completed â€“ Tested on Azure  
ğŸ§ª Data Source: NYC Taxi Dataset  
ğŸ“ File Format: Parquet + Delta  
